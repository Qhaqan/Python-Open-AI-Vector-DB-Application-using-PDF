{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obgBZWEdQx0U",
        "outputId": "ed0e4239-fcd2-4bd7-a112-3ed07b88ce66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.7.4)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.0.3-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.32 (from langchain)\n",
            "  Downloading langchain_core-0.2.34-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.104-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.32->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.2.14-py3-none-any.whl (997 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading langchain_community-0.2.12-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.2.34-py3-none-any.whl (393 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.104-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-1.0.3-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, python-dotenv, PyPDF2, pypdf, pinecone-plugin-interface, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, tiktoken, pinecone-plugin-inference, jsonpatch, httpcore, pinecone-client, openai, httpx, dataclasses-json, langsmith, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed PyPDF2-3.0.1 dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.14 langchain-core-0.2.34 langchain-text-splitters-0.2.2 langchain_community-0.2.12 langsmith-0.1.104 marshmallow-3.22.0 mypy-extensions-1.0.0 openai-0.28.0 orjson-3.10.7 pinecone-client-5.0.1 pinecone-plugin-inference-1.0.3 pinecone-plugin-interface-0.0.7 pypdf-4.3.1 python-dotenv-1.0.1 tenacity-8.5.0 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 numpy transformers tiktoken pinecone-client pypdf PyPDF2 openai==0.28 langchain pandas  numpy python-dotenv langchain_community\n",
        "# openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuaN5AwATa4T",
        "outputId": "035eb64c-ccef-4e01-cbe7-caca28b79da9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import langchain\n",
        "import pinecone\n",
        "import nltk\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "#from langchain.vectorstores import Pinecone\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t59vdhPIoqKR",
        "outputId": "3e3b72c6-c88a-456a-a63a-1c55ddbd689e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, async_client=None, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.environ['OPENAI_API_KEY'] = 'sk-proj-bUQD2DxxxxxxxxxxxxxxxxlbkFJ_4fLaliG5A0p6DPEYCNjKT8p6nDYFEV4xxxxxxxxxxxxxA'\n",
        "os.environ['PINECONE_ENVIRONMENT'] = 'us-west1-gcp'\n",
        "os.environ['PINECONE_API_KEY'] = '5a6xxxxxxxxxxxxxxxxxxx37'\n",
        "openai.api_key='sk-proj-bUQD2Dvxxxxxxxxxxx1mVdZQ2Fwby9NT3BlbkFJ_4fLaliG5AxxxxxxxxxxxxxxxxeuvNpcoA'\n",
        "embeddings=OpenAIEmbeddings(api_key=os.environ['OPENAI_API_KEY'])\n",
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aytZE8ENpDBZ",
        "outputId": "6f1766d0-d2e4-4623-bddf-e48aae8d9541"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectors=embeddings.embed_query(\"How many open accounts?\")\n",
        "len(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVZspMFEpT73",
        "outputId": "eec0c453-a90a-4483-f913-7138d0ff4244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "us-west1-gcp\n"
          ]
        }
      ],
      "source": [
        "## Vector Search DB In Pinecone\n",
        "\n",
        "index_name=\"langchainvector\"\n",
        "#index=Pinecone.from_documents(doc,embeddings,index_name=index_name)\n",
        "#pinecone.init(api_key=os.environ['PINECONE_API_KEY'], environment=os.environ['PINECONE_ENVIRONMENT'])\n",
        "#pc=Pinecone(api_key='5a62d5e4-eaa7-4e7d-ad4d-39a310016337')\n",
        "print(os.environ.get(\"PINECONE_ENVIRONMENT\"))\n",
        "pc = Pinecone(\n",
        "        api_key=os.environ.get(\"PINECONE_API_KEY\"),\n",
        "        environment=os.environ.get(\"PINECONE_ENVIRONMENT\"),\n",
        "        index_name=index_name\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Now do stuff\n",
        "if index_name not in pc.list_indexes().names():\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1536,\n",
        "            metric='cosine',\n",
        "            spec=ServerlessSpec(\n",
        "                cloud='aws',\n",
        "                region='us-east-1'\n",
        "            )\n",
        "        )\n",
        "\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IHlUi263B0k7"
      },
      "outputs": [],
      "source": [
        "# Step 3: Function to extract text from a PDF file\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rm3fEduzDY2a"
      },
      "outputs": [],
      "source": [
        "# Step 4: Function to split text into manageable chunks\n",
        "def split_text(text, max_tokens=200):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    current_tokens = 0\n",
        "    for sentence in sentences:\n",
        "        sentence_tokens = len(sentence.split())\n",
        "        if current_tokens + sentence_tokens > max_tokens:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_tokens = 0\n",
        "\n",
        "        current_chunk.append(sentence)\n",
        "        current_tokens += sentence_tokens\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gSDPRqO_DkDi"
      },
      "outputs": [],
      "source": [
        "# Step 4: Function to convert text to embeddings using OpenAI\n",
        "def get_embeddings(text):\n",
        "    response = openai.Embedding.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-3-small\"  # text-embedding-3-large && text-embedding-3-small Replace with the OpenAI model you want to use\n",
        "    )\n",
        "    embedding = response['data'][0]['embedding']\n",
        "    #print((len(embedding)))\n",
        "    # Validate the embedding\n",
        "    if len(embedding) != 1536:\n",
        "        print(f\"Embedding dimension is incorrect: {len(embedding)}\")\n",
        "        print(\"Embedding:\", embedding)  # This will print the actual embedding if the dimension is incorrect\n",
        "        raise ValueError(f\"Expected embedding dimension 1536, but got {len(embedding)}\")\n",
        "    else:\n",
        "        print(\"Its 1536 embed \\n\")\n",
        "\n",
        "    if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):\n",
        "        raise ValueError(\"Embedding contains NaN or infinite values\")\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nHigq57UDwQq"
      },
      "outputs": [],
      "source": [
        "# Step 5: Function to upsert data into Pinecone\n",
        "def upsert_to_pinecone(embedding, metadata, document_id):\n",
        "    upsert_data = [\n",
        "        {\n",
        "            \"id\": document_id,\n",
        "            \"values\": embedding,\n",
        "            \"metadata\": metadata\n",
        "        }\n",
        "    ]\n",
        "    #print(upsert_data)\n",
        "    index.upsert(upsert_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Fl0DOD9eEefE"
      },
      "outputs": [],
      "source": [
        "# Step 6: Process and store PDF embeddings in Pinecone\n",
        "def process_and_store_pdf(pdf_path, document_id):\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    chunks = split_text(text, max_tokens=200)\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        embedding = get_embeddings(chunk)\n",
        "        chunk_id = f\"{document_id}_chunk_{i}\"\n",
        "        metadata = {\"source\": pdf_path, \"chunk\": i,\"text\": chunk}\n",
        "        print(len(embedding),\"\\n\\n\")\n",
        "        upsert_to_pinecone(embedding, metadata, chunk_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u-QpsuIE2Gx",
        "outputId": "c4ad2038-a2a5-45d5-d980-6cb478d54831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n",
            "Its 1536 embed \n",
            "\n",
            "1536 \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Example usage\n",
        "pdf_path = \"/content/budget_speech.pdf\"  # Replace with your PDF file path\n",
        "document_id = \"doc1\"  # Unique identifier for the document\n",
        "process_and_store_pdf(pdf_path, document_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7ZQUCGXQfC7",
        "outputId": "53d258e0-0cbc-42cd-905f-83d84f49d262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To increase value addition in the domestic electronics industry,  \n",
            "I propose to remove the BCD, subject to conditions, on oxygen free copper \n",
            "for manufacture of resistors. I also propose to exempt certain parts for manufacture of connectors. Chemicals and Petrochemicals  \n",
            "131. To support existing and new capacities in the pipeline, I propose to \n",
            "increas e the BCD on ammonium nitrate  from 7.5 to 10 per cent. Plastics  \n",
            "132. PVC flex banners are non -biodegradable and hazardous for \n",
            "environment and health. To curb their imports, I propose to raise the BCD on \n",
            "them from 10 to 25 per cent. Telecommunication Equipment \n",
            "133. To incentivise domestic manufacturing, I propose to increase the BCD  \n",
            "from 10 to 15 per cent on PCBA of specified telecom equipment. Trade facilitation  \n",
            "134. To promote domestic aviation and boat & ship MRO, I propose to  \n",
            "extend the period for  export of goods imported for repairs from six months \n",
            "to one year. In the same vein, I propose to extend the time -limit for re -import \n",
            "of goods for repairs under warranty from three to five years. 25  \n",
            " \n",
            "Direct Taxes  \n",
            " \n",
            "135.\n",
            "These proposals are proposed to be given effect with immediate \n",
            "force. 53  \n",
            " \n",
            "C.5    Rationalis ation of tax deducted at source (TDS) rates:  It is \n",
            "proposed to bring down TDS rates from 5 per cent to 2 per cent in \n",
            "certain sections and omit section 194F where TDS rate is 20 per \n",
            "cent, as given below:  \n",
            "C.6 Credit of TDC and TCS: It is proposed to allow credit of all tax \n",
            "deducted or collected while computing the amount of tax to be deducted on salary income under section 192. C.7 Claiming credit for TCS of minor in the hands of parent: It is \n",
            "proposed to empower the Board to make rules to provide credit \n",
            "of tax collected to person other than collectee.\n",
            "E.7 Lower deduction / collection certificate of tax at source:  It is \n",
            "proposed to allow for application for lower deduction / collection certificate o f tax for section 194Q (TDS on payment for purchase \n",
            "of goods) and sub- section (1H) of section 206C (TCS on receipt of \n",
            "sale of goods). E.8  Notification of certain persons or class of persons as exempt \n",
            "from TCS:  It is proposed to empower the government to notify \n",
            "persons or class of persons from whom no collection of tax shall \n",
            "be made or collection of tax shall be made at a lower rate in \n",
            "respect of specified transactions. E.9 Time limit to file correction statement for TDS/TCS statements:  \n",
            "It is proposed to p rovide that no correction statement shall be \n",
            "delivered after the expiry of six years from the end of the financial  57  \n",
            " \n",
            "year in which the TDS/TCS statement are respectively required to \n",
            "be delivered. E.10  Penalty for failure to furnish statements : It is proposed to \n",
            "provide for penalty on late furnishing of TDS or TCS statement \n",
            "beyond one month instead of the existing period of 12 months.\n",
            "Section  Present \n",
            "TDS \n",
            "Rate  Proposed \n",
            "TDS Rate  With effect \n",
            "from  \n",
            "Section 194D - Payment of insurance \n",
            "commission (in case of person other \n",
            "than company)  5% 2% 1.4.2025  \n",
            "Section 194DA - Payment in respect \n",
            "of life insurance policy  5% 2% 1.10.2024  \n",
            "Section 194G – Commission etc on \n",
            "sale of lottery tickets  5% 2% 1.10.2024  \n",
            "Section 194H - Payment of \n",
            "commission or brokerage  5% 2% 1.10.2024  \n",
            "Section 194 -IB - Payment of rent by \n",
            "individual or HUF  5% 2% 1.10.2024  \n",
            "Section 194M - Payment of certain \n",
            "sums by certain individuals or Hindu \n",
            "undivided family  5% 2% 1.10.2024  \n",
            "Section 194 -O - Payment of certain \n",
            "sums by e -commerce operator to e-\n",
            "commerce participant  1% 0.1%  1.10.2024  \n",
            "Section 194F relating to payments \n",
            "on account of repurchase of units by Mutual Fund or Unit Trust of India  Proposed to be \n",
            "omitted  1.10.2024   54  \n",
            " \n",
            "C.8 Alignment of interest rate on delayed payment on TCS with TDS : \n",
            "It is proposed to increase  the rate of simple interest from 1 per \n",
            "cent to 1.5 per cent on delayed payments of TCS after collection, \n",
            "as in the case of TDS.\n",
            "Increase  in duration  for re -import of goods exported out of India  \n",
            "The time -period of duty free re -import of goods  (other than those \n",
            "under export promotion scheme s) exported out under warranty from \n",
            "India has been increased from 3 years to 5 years , further extendable \n",
            "by 2 years . 49  \n",
            " \n",
            "E.2. Increase in duration for export of articles of foreign origin imported \n",
            "into India for repairs  \n",
            "Currently, articles of foreign origin  can be imported into India  for \n",
            "repairs  subject to their re -exportation within six months extendable \n",
            "upto 1 year. The duration for export in the case of aircraft and vessels \n",
            "imported for maintenance, repair and overhauling has been \n",
            "increased  from 6 months to 1 year,  further extendable by 1 year. F. OTHERS \n",
            "There are few other changes of minor nature. For details of the budget \n",
            "proposals, the Explanatory Memorandum and other relevant budget documents may be referred to.\n"
          ]
        }
      ],
      "source": [
        "query_vector = embeddings.embed_query(\"Key points of Indian budget?\")\n",
        "#print(query_vector)\n",
        "response = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=5,\n",
        "    include_values=True,  # or False if you don't need the vector values in the response\n",
        "    include_metadata=True  # or False if you don't need metadata in the response\n",
        ")\n",
        "similar_docs = response['matches']\n",
        "for doc in similar_docs:\n",
        "   print(doc['metadata']['text'])   # Assuming your documents have 'text' metadata"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
